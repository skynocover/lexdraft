import { eq } from 'drizzle-orm'
import { nanoid } from 'nanoid'
import { getDB } from '../db'
import { files, briefs, disputes, damages, lawRefs, timelineEvents as timelineEventsTable } from '../db/schema'
import { searchLaw } from '../lib/lawSearch'
import { callClaudeWithCitations, type ClaudeDocument } from './claudeClient'
import { callAIStreaming, type AIEnv } from './aiClient'
import type { ToolDef } from './aiClient'
import type { SSEEvent } from '../../shared/types'
import type { Paragraph } from '../../client/stores/useBriefStore'

/**
 * Collect full text from an SSE streaming response.
 * Flushes the TextDecoder and strips U+FFFD replacement characters
 * that appear when multi-byte UTF-8 chars are split across chunks.
 */
async function collectStreamText(response: Response): Promise<string> {
  let text = ''
  const reader = response.body!.getReader()
  const decoder = new TextDecoder()
  let buffer = ''

  while (true) {
    const { done, value } = await reader.read()
    if (done) break
    buffer += decoder.decode(value, { stream: true })
    const lines = buffer.split('\n')
    buffer = lines.pop() || ''
    for (const line of lines) {
      if (!line.startsWith('data: ')) continue
      const data = line.slice(6).trim()
      if (data === '[DONE]') continue
      try {
        const chunk = JSON.parse(data)
        const content = chunk.choices?.[0]?.delta?.content
        if (content) text += content
      } catch { /* skip */ }
    }
  }
  // Flush remaining bytes in decoder
  buffer += decoder.decode()
  if (buffer) {
    const lines = buffer.split('\n')
    for (const line of lines) {
      if (!line.startsWith('data: ')) continue
      const data = line.slice(6).trim()
      if (data === '[DONE]') continue
      try {
        const chunk = JSON.parse(data)
        const content = chunk.choices?.[0]?.delta?.content
        if (content) text += content
      } catch { /* skip */ }
    }
  }

  // Strip U+FFFD replacement characters from corrupted multi-byte sequences
  return text.replace(/\uFFFD/g, '')
}

// Tool definitions in OpenAI function calling format
export const TOOL_DEFINITIONS: ToolDef[] = [
  {
    type: 'function',
    function: {
      name: 'list_files',
      description: 'åˆ—å‡ºæ¡ˆä»¶æ‰€æœ‰æª”æ¡ˆï¼ŒåŒ…å« idã€filenameã€categoryã€statusã€summaryã€‚ç”¨æ–¼äº†è§£æ¡ˆä»¶æœ‰å“ªäº›å·å®—æ–‡ä»¶ã€‚',
      parameters: {
        type: 'object',
        properties: {},
        required: [],
      },
    },
  },
  {
    type: 'function',
    function: {
      name: 'read_file',
      description: 'è®€å–æŒ‡å®šæª”æ¡ˆçš„å…¨æ–‡å…§å®¹ï¼ˆæˆªæ–· 15000 å­—ï¼‰ã€‚éœ€è¦å…ˆç”¨ list_files å–å¾—æª”æ¡ˆ idã€‚',
      parameters: {
        type: 'object',
        properties: {
          file_id: {
            type: 'string',
            description: 'è¦è®€å–çš„æª”æ¡ˆ ID',
          },
        },
        required: ['file_id'],
      },
    },
  },
  {
    type: 'function',
    function: {
      name: 'write_brief_section',
      description: 'æ’°å¯«æ›¸ç‹€çš„ä¸€å€‹æ®µè½ã€‚ä½¿ç”¨ Claude Citations API å¾ä¾†æºæ–‡ä»¶ä¸­æå–å¼•ç”¨ã€‚éœ€è¦å…ˆç”¨ list_files æ‰¾åˆ°ç›¸é—œæª”æ¡ˆã€‚',
      parameters: {
        type: 'object',
        properties: {
          brief_id: {
            type: 'string',
            description: 'æ›¸ç‹€ ID',
          },
          section: {
            type: 'string',
            description: 'æ®µè½æ‰€å±¬ç« ç¯€ï¼ˆå¦‚ã€Œå£¹ã€å‰è¨€ã€ã€ã€Œè²³ã€å°±è¢«å‘Šå„é …æŠ—è¾¯ä¹‹åé§ã€ï¼‰',
          },
          subsection: {
            type: 'string',
            description: 'å­ç« ç¯€æ¨™é¡Œï¼ˆå¦‚ã€Œä¸€ã€é—œæ–¼è²¨ç‰©ç‘•ç–µä¹‹æŠ—è¾¯ã€ï¼‰ï¼Œç„¡å‰‡ç•™ç©ºå­—ä¸²',
          },
          instruction: {
            type: 'string',
            description: 'æ’°å¯«æŒ‡ç¤ºï¼Œèªªæ˜é€™å€‹æ®µè½è¦è¡¨é”ä»€éº¼è«–é»',
          },
          relevant_file_ids: {
            type: 'array',
            items: { type: 'string' },
            description: 'ç›¸é—œä¾†æºæª”æ¡ˆçš„ ID åˆ—è¡¨',
          },
          dispute_id: {
            type: 'string',
            description: 'é—œè¯çš„çˆ­é» IDï¼ˆå¯é¸ï¼‰',
          },
        },
        required: ['brief_id', 'section', 'subsection', 'instruction', 'relevant_file_ids'],
      },
    },
  },
  {
    type: 'function',
    function: {
      name: 'create_brief',
      description: 'å»ºç«‹ä¸€ä»½æ–°çš„æ›¸ç‹€ã€‚æ’°å¯«æ›¸ç‹€å‰å¿…é ˆå…ˆå‘¼å«æ­¤å·¥å…·å–å¾— brief_idï¼Œå†ç”¨ write_brief_section é€æ®µæ’°å¯«ã€‚',
      parameters: {
        type: 'object',
        properties: {
          brief_type: {
            type: 'string',
            enum: ['complaint', 'defense', 'preparation', 'appeal'],
            description: 'æ›¸ç‹€é¡å‹ï¼šcomplaint èµ·è¨´ç‹€ã€defense ç­”è¾¯ç‹€ã€preparation æº–å‚™æ›¸ç‹€ã€appeal ä¸Šè¨´ç‹€',
          },
          title: {
            type: 'string',
            description: 'æ›¸ç‹€æ¨™é¡Œï¼ˆå¦‚ã€Œæ°‘äº‹æº–å‚™äºŒç‹€ã€ã€ã€Œæ°‘äº‹ç­”è¾¯ç‹€ã€ï¼‰',
          },
        },
        required: ['brief_type', 'title'],
      },
    },
  },
  {
    type: 'function',
    function: {
      name: 'analyze_disputes',
      description: 'åˆ†ææ¡ˆä»¶æ‰€æœ‰æª”æ¡ˆï¼Œè­˜åˆ¥é›™æ–¹çˆ­é»ã€‚æœƒè‡ªå‹•è¼‰å…¥æ‰€æœ‰å·²è™•ç†å®Œæˆçš„æª”æ¡ˆæ‘˜è¦å’Œä¸»å¼µï¼Œåˆ†æå¾Œå¯«å…¥çˆ­é»è³‡æ–™åº«ã€‚',
      parameters: {
        type: 'object',
        properties: {},
        required: [],
      },
    },
  },
  {
    type: 'function',
    function: {
      name: 'calculate_damages',
      description: 'åˆ†ææ¡ˆä»¶æ–‡ä»¶ï¼Œè¨ˆç®—å„é …è«‹æ±‚é‡‘é¡æ˜ç´°ã€‚æœƒè‡ªå‹•è¼‰å…¥æ‰€æœ‰å·²è™•ç†å®Œæˆçš„æª”æ¡ˆæ‘˜è¦ï¼ˆå« key_amountsï¼‰ï¼Œåˆ†æå¾Œå¯«å…¥é‡‘é¡è³‡æ–™åº«ã€‚',
      parameters: {
        type: 'object',
        properties: {},
        required: [],
      },
    },
  },
  {
    type: 'function',
    function: {
      name: 'search_law',
      description: 'æœå°‹æ³•è¦æ¢æ–‡ã€‚æ”¯æ´æ³•è¦åç¨±ï¼ˆå¦‚ã€Œæ°‘æ³•ã€ï¼‰ã€ç‰¹å®šæ¢è™Ÿï¼ˆå¦‚ã€Œæ°‘æ³•ç¬¬184æ¢ã€ï¼‰ã€æ³•å¾‹æ¦‚å¿µï¼ˆå¦‚ã€Œæå®³è³ å„Ÿã€ï¼‰ç­‰æœå°‹æ–¹å¼ã€‚æœå°‹çµæœæœƒè‡ªå‹•å¯«å…¥æ¡ˆä»¶æ³•æ¢å¼•ç”¨åˆ—è¡¨ã€‚',
      parameters: {
        type: 'object',
        properties: {
          query: {
            type: 'string',
            description: 'æœå°‹é—œéµå­—ï¼Œä¾‹å¦‚ã€Œæ°‘æ³•ç¬¬184æ¢ã€æˆ–ã€Œæå®³è³ å„Ÿã€',
          },
          limit: {
            type: 'number',
            description: 'å›å‚³ç­†æ•¸ä¸Šé™ï¼Œé è¨­ 10',
          },
        },
        required: ['query'],
      },
    },
  },
  {
    type: 'function',
    function: {
      name: 'generate_timeline',
      description: 'åˆ†ææ¡ˆä»¶æ‰€æœ‰æª”æ¡ˆï¼Œç”¢ç”Ÿæ™‚é–“è»¸äº‹ä»¶åˆ—è¡¨ã€‚è‡ªå‹•è¼‰å…¥æ‰€æœ‰å·²è™•ç†å®Œæˆçš„æª”æ¡ˆæ‘˜è¦ï¼Œåˆ†ææ—¥æœŸã€äº‹ä»¶ç­‰æ™‚é–“ç›¸é—œè³‡è¨Šï¼Œç”¢ç”Ÿæ™‚é–“è»¸ä¾›å¾‹å¸«åƒè€ƒã€‚',
      parameters: {
        type: 'object',
        properties: {},
        required: [],
      },
    },
  },
]

interface ExecuteToolContext {
  sendSSE: (event: SSEEvent) => Promise<void>
  aiEnv: AIEnv
  mongoUrl: string
}

// Tool execution
export async function executeTool(
  toolName: string,
  args: Record<string, unknown>,
  caseId: string,
  db: D1Database,
  ctx?: ExecuteToolContext,
): Promise<{ result: string; success: boolean }> {
  const drizzle = getDB(db)

  switch (toolName) {
    case 'list_files': {
      const rows = await drizzle
        .select({
          id: files.id,
          filename: files.filename,
          category: files.category,
          status: files.status,
          doc_type: files.doc_type,
          doc_date: files.doc_date,
          summary: files.summary,
        })
        .from(files)
        .where(eq(files.case_id, caseId))

      const list = rows.map((f) => ({
        id: f.id,
        filename: f.filename,
        category: f.category,
        status: f.status,
        doc_type: f.doc_type,
        doc_date: f.doc_date,
        summary: f.summary ? JSON.parse(f.summary) : null,
      }))

      return {
        result: JSON.stringify(list, null, 2),
        success: true,
      }
    }

    case 'read_file': {
      const fileId = args.file_id as string
      if (!fileId) {
        return { result: 'Error: file_id is required', success: false }
      }

      const rows = await drizzle
        .select({
          id: files.id,
          filename: files.filename,
          full_text: files.full_text,
          category: files.category,
          doc_type: files.doc_type,
        })
        .from(files)
        .where(eq(files.id, fileId))

      if (!rows.length) {
        return { result: `Error: file not found (id: ${fileId})`, success: false }
      }

      const file = rows[0]
      const text = file.full_text || 'ï¼ˆç„¡æ–‡å­—å…§å®¹ï¼‰'
      const truncated = text.length > 15000 ? text.slice(0, 15000) + '\n\n... [æˆªæ–·ï¼Œå…± ' + text.length + ' å­—]' : text

      return {
        result: `æª”æ¡ˆï¼š${file.filename}\nåˆ†é¡ï¼š${file.category}\né¡å‹ï¼š${file.doc_type}\n\nå…¨æ–‡å…§å®¹ï¼š\n${truncated}`,
        success: true,
      }
    }

    case 'create_brief': {
      const briefType = args.brief_type as string
      const title = args.title as string

      if (!briefType || !title) {
        return { result: 'Error: brief_type and title are required', success: false }
      }

      const briefId = nanoid()
      const now = new Date().toISOString()

      await drizzle.insert(briefs).values({
        id: briefId,
        case_id: caseId,
        brief_type: briefType,
        title,
        content_structured: JSON.stringify({ paragraphs: [] }),
        version: 1,
        created_at: now,
        updated_at: now,
      })

      // Notify frontend so it can load the new brief
      if (ctx) {
        await ctx.sendSSE({
          type: 'brief_update',
          brief_id: briefId,
          action: 'create_brief',
          data: {
            id: briefId,
            case_id: caseId,
            brief_type: briefType,
            title,
            content_structured: { paragraphs: [] },
            version: 1,
            created_at: now,
            updated_at: now,
          },
        })
      }

      return {
        result: `å·²å»ºç«‹æ›¸ç‹€ã€Œ${title}ã€ï¼Œbrief_id: ${briefId}ã€‚è«‹ä½¿ç”¨æ­¤ brief_id æ­é… write_brief_section é€æ®µæ’°å¯«å…§å®¹ã€‚`,
        success: true,
      }
    }

    case 'write_brief_section': {
      if (!ctx) {
        return { result: 'Error: missing execution context', success: false }
      }

      const briefId = args.brief_id as string
      const section = args.section as string
      const subsection = (args.subsection as string) || ''
      const instruction = args.instruction as string
      const relevantFileIds = args.relevant_file_ids as string[]
      const disputeId = (args.dispute_id as string) || null

      if (!briefId || !section || !instruction || !relevantFileIds?.length) {
        return { result: 'Error: brief_id, section, instruction, relevant_file_ids are required', success: false }
      }

      // 1. Load file contents for document blocks
      const fileRows = await drizzle
        .select({
          id: files.id,
          filename: files.filename,
          full_text: files.full_text,
        })
        .from(files)
        .where(eq(files.case_id, caseId))

      const relevantFiles = fileRows.filter((f) => relevantFileIds.includes(f.id))
      if (!relevantFiles.length) {
        return { result: 'Error: no matching files found', success: false }
      }

      const documents: ClaudeDocument[] = relevantFiles.map((f) => ({
        title: f.filename,
        content: (f.full_text || '').slice(0, 20000),
        file_id: f.id,
      }))

      // 2. Call Claude with Citations
      const claudeInstruction = `ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å°ç£å¾‹å¸«åŠ©ç†ã€‚è«‹æ ¹æ“šæä¾›çš„ä¾†æºæ–‡ä»¶ï¼Œæ’°å¯«æ³•å¾‹æ›¸ç‹€çš„ä¸€å€‹æ®µè½ã€‚

æ’°å¯«è¦æ±‚ï¼š
- ç« ç¯€ï¼š${section}
- å­ç« ç¯€ï¼š${subsection || 'ï¼ˆç„¡ï¼‰'}
- æŒ‡ç¤ºï¼š${instruction}

æ’°å¯«è¦å‰‡ï¼š
- ä½¿ç”¨æ­£å¼æ³•å¾‹æ–‡æ›¸ç”¨èªï¼ˆç¹é«”ä¸­æ–‡ï¼‰
- è«–è¿°è¦æœ‰é‚è¼¯ã€æ¢ç†åˆ†æ˜
- å¦‚æœ‰æ³•æ¢å¼•ç”¨ï¼Œéœ€æ­£ç¢ºæ¨™ç¤ºæ³•æ¢åç¨±èˆ‡æ¢è™Ÿ
- çµ•å°ä¸è¦è¼¸å‡ºä»»ä½• XML æ¨™ç±¤ï¼ˆå¦‚ <document_context> ç­‰ï¼‰
- çµ•å°ä¸è¦ä½¿ç”¨ emoji æˆ–ç‰¹æ®Šç¬¦è™Ÿ
- ç›´æ¥æ’°å¯«æ®µè½å…§å®¹ï¼Œä¸éœ€è¦åŠ å…¥ç« ç¯€æ¨™é¡Œ
- æ®µè½é•·åº¦æ§åˆ¶åœ¨ 150-300 å­—ä¹‹é–“ï¼Œç°¡æ½”æœ‰åŠ›`

      const { text, segments, citations } = await callClaudeWithCitations(
        ctx.aiEnv,
        documents,
        claudeInstruction,
      )

      // 3. Build Paragraph object
      const paragraph: Paragraph = {
        id: nanoid(),
        section,
        subsection,
        content_md: text,
        segments,
        dispute_id: disputeId,
        citations,
      }

      // 4. Read current brief content and append
      const briefRows = await drizzle
        .select()
        .from(briefs)
        .where(eq(briefs.id, briefId))

      if (!briefRows.length) {
        return { result: `Error: brief not found (id: ${briefId})`, success: false }
      }

      const brief = briefRows[0]
      let contentStructured: { paragraphs: Paragraph[] } = { paragraphs: [] }
      if (brief.content_structured) {
        try {
          contentStructured = JSON.parse(brief.content_structured)
        } catch {
          contentStructured = { paragraphs: [] }
        }
      }

      contentStructured.paragraphs.push(paragraph)

      // 5. Update DB
      await drizzle
        .update(briefs)
        .set({
          content_structured: JSON.stringify(contentStructured),
          updated_at: new Date().toISOString(),
        })
        .where(eq(briefs.id, briefId))

      // 6. Send SSE brief_update
      await ctx.sendSSE({
        type: 'brief_update',
        brief_id: briefId,
        action: 'add_paragraph',
        data: paragraph,
      })

      return {
        result: `å·²æ’°å¯«æ®µè½ã€Œ${section}${subsection ? ' > ' + subsection : ''}ã€ï¼ŒåŒ…å« ${citations.length} å€‹å¼•ç”¨ã€‚`,
        success: true,
      }
    }

    case 'analyze_disputes': {
      if (!ctx) {
        return { result: 'Error: missing execution context', success: false }
      }

      // 1. Load all ready files with summaries
      const fileRows = await drizzle
        .select({
          id: files.id,
          filename: files.filename,
          category: files.category,
          summary: files.summary,
          extracted_claims: files.extracted_claims,
        })
        .from(files)
        .where(eq(files.case_id, caseId))

      const readyFiles = fileRows.filter((f) => f.summary)
      if (!readyFiles.length) {
        return { result: 'æ²’æœ‰å·²è™•ç†å®Œæˆçš„æª”æ¡ˆï¼Œè«‹å…ˆä¸Šå‚³ä¸¦ç­‰å¾…æª”æ¡ˆè™•ç†å®Œç•¢ã€‚', success: false }
      }

      // Build context for Gemini
      const fileContext = readyFiles.map((f) => {
        const summary = f.summary ? JSON.parse(f.summary) : {}
        const claims = f.extracted_claims ? JSON.parse(f.extracted_claims) : []
        return `ã€${f.filename}ã€‘(${f.category})\næ‘˜è¦ï¼š${summary.summary || 'ç„¡'}\nä¸»å¼µï¼š${claims.length > 0 ? claims.join('ï¼›') : 'ç„¡'}`
      }).join('\n\n')

      // 2. Call Gemini for dispute analysis
      const analysisPrompt = `ä½ æ˜¯å°ˆæ¥­çš„å°ç£æ³•å¾‹åˆ†æåŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹æ¡ˆä»¶æ–‡ä»¶æ‘˜è¦ï¼Œåˆ†æé›™æ–¹çš„çˆ­é»ã€‚

${fileContext}

è«‹ä»¥ JSON æ ¼å¼å›å‚³çˆ­é»åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
  {
    "number": 1,
    "title": "çˆ­é»æ¨™é¡Œ",
    "our_position": "æˆ‘æ–¹ç«‹å ´",
    "their_position": "å°æ–¹ç«‹å ´",
    "evidence": ["ç›¸é—œè­‰æ“š1", "ç›¸é—œè­‰æ“š2"],
    "law_refs": ["æ°‘æ³•ç¬¬XXXæ¢"],
    "priority": 1
  }
]

é‡è¦ï¼šçµ•å°ä¸è¦ä½¿ç”¨ emoji æˆ–ç‰¹æ®Šç¬¦è™Ÿï¼ˆå¦‚ âœ…âŒğŸ”·ğŸ“„âš–ï¸ğŸ’°ğŸ”¨ ç­‰ï¼‰ï¼Œåªç”¨ç´”ä¸­æ–‡æ–‡å­—å’Œæ¨™é»ç¬¦è™Ÿã€‚
åªå›å‚³ JSON é™£åˆ—ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚`

      const aiResponse = await callAIStreaming(ctx.aiEnv, {
        messages: [
          { role: 'system', content: 'ä½ æ˜¯å°ˆæ¥­çš„å°ç£æ³•å¾‹åˆ†æåŠ©æ‰‹ã€‚' },
          { role: 'user', content: analysisPrompt },
        ],
      })

      const responseText = await collectStreamText(aiResponse)

      // 3. Parse disputes from response
      let disputeList: Array<{
        number: number
        title: string
        our_position: string
        their_position: string
        evidence: string[]
        law_refs: string[]
        priority: number
      }> = []

      try {
        // Extract JSON from response (might be wrapped in markdown code blocks)
        const jsonMatch = responseText.match(/\[[\s\S]*\]/)
        if (jsonMatch) {
          disputeList = JSON.parse(jsonMatch[0])
        }
      } catch {
        return { result: 'Error: ç„¡æ³•è§£æçˆ­é»åˆ†æçµæœ', success: false }
      }

      if (!disputeList.length) {
        return { result: 'æœªèƒ½è­˜åˆ¥å‡ºçˆ­é»ï¼Œè«‹ç¢ºèªæª”æ¡ˆå·²æ­£ç¢ºè™•ç†ã€‚', success: false }
      }

      // 4. Clear old disputes for this case, then write new ones
      await drizzle.delete(disputes).where(eq(disputes.case_id, caseId))

      const disputeRecords = disputeList.map((d) => ({
        id: nanoid(),
        case_id: caseId,
        number: d.number,
        title: d.title,
        our_position: d.our_position,
        their_position: d.their_position,
        evidence: JSON.stringify(d.evidence || []),
        law_refs: JSON.stringify(d.law_refs || []),
        priority: d.priority || 0,
      }))

      for (const record of disputeRecords) {
        await drizzle.insert(disputes).values(record)
      }

      // 5. Send SSE brief_update
      await ctx.sendSSE({
        type: 'brief_update',
        brief_id: '',
        action: 'set_disputes',
        data: disputeRecords,
      })

      // 6. Return summary
      const summary = disputeRecords
        .map((d) => `${d.number}. ${d.title}`)
        .join('\n')

      return {
        result: `å·²è­˜åˆ¥ ${disputeRecords.length} å€‹çˆ­é»ï¼š\n${summary}`,
        success: true,
      }
    }

    case 'calculate_damages': {
      if (!ctx) {
        return { result: 'Error: missing execution context', success: false }
      }

      // 1. Load all ready files with summaries
      const damageFileRows = await drizzle
        .select({
          id: files.id,
          filename: files.filename,
          category: files.category,
          summary: files.summary,
          extracted_claims: files.extracted_claims,
        })
        .from(files)
        .where(eq(files.case_id, caseId))

      const damageReadyFiles = damageFileRows.filter((f) => f.summary)
      if (!damageReadyFiles.length) {
        return { result: 'æ²’æœ‰å·²è™•ç†å®Œæˆçš„æª”æ¡ˆï¼Œè«‹å…ˆä¸Šå‚³ä¸¦ç­‰å¾…æª”æ¡ˆè™•ç†å®Œç•¢ã€‚', success: false }
      }

      // Build context for AI
      const damageFileContext = damageReadyFiles.map((f) => {
        const summary = f.summary ? JSON.parse(f.summary) : {}
        const claims = f.extracted_claims ? JSON.parse(f.extracted_claims) : []
        return `ã€${f.filename}ã€‘(${f.category})\næ‘˜è¦ï¼š${summary.summary || 'ç„¡'}\né‡‘é¡ï¼š${summary.key_amounts ? JSON.stringify(summary.key_amounts) : 'ç„¡'}\nä¸»å¼µï¼š${claims.length > 0 ? claims.join('ï¼›') : 'ç„¡'}`
      }).join('\n\n')

      // 2. Call AI for damage analysis
      const damagePrompt = `ä½ æ˜¯å°ˆæ¥­çš„å°ç£æ³•å¾‹åˆ†æåŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹æ¡ˆä»¶æ–‡ä»¶æ‘˜è¦ï¼Œè¨ˆç®—å„é …è«‹æ±‚é‡‘é¡æ˜ç´°ã€‚

${damageFileContext}

è«‹ä»¥ JSON æ ¼å¼å›å‚³é‡‘é¡é …ç›®åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
  {
    "category": "è²¨æ¬¾",
    "description": "åˆç´„è²¨æ¬¾å°¾æ¬¾",
    "amount": 1200000,
    "basis": "ä¾ç³»çˆ­è²·è³£åˆç´„ç¬¬5æ¢",
    "evidence_refs": ["åŸè­‰äºŒ"]
  }
]

é‡‘é¡ category å¸¸è¦‹åˆ†é¡ï¼šè²¨æ¬¾ã€åˆ©æ¯ã€é•ç´„é‡‘ã€ç²¾ç¥æ…°æ’«é‡‘ã€æå®³è³ å„Ÿã€å…¶ä»–ã€‚
amount ç‚ºæ•´æ•¸ï¼Œä»¥æ–°å°å¹£å…ƒè¨ˆã€‚
é‡è¦ï¼šçµ•å°ä¸è¦ä½¿ç”¨ emoji æˆ–ç‰¹æ®Šç¬¦è™Ÿï¼ˆå¦‚ âœ…âŒğŸ”·ğŸ“„âš–ï¸ğŸ’°ğŸ”¨ ç­‰ï¼‰ï¼Œåªç”¨ç´”ä¸­æ–‡æ–‡å­—å’Œæ¨™é»ç¬¦è™Ÿã€‚
åªå›å‚³ JSON é™£åˆ—ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚`

      const damageAiResponse = await callAIStreaming(ctx.aiEnv, {
        messages: [
          { role: 'system', content: 'ä½ æ˜¯å°ˆæ¥­çš„å°ç£æ³•å¾‹åˆ†æåŠ©æ‰‹ã€‚' },
          { role: 'user', content: damagePrompt },
        ],
      })

      const damageResponseText = await collectStreamText(damageAiResponse)

      // 3. Parse damages from response
      let damageList: Array<{
        category: string
        description: string
        amount: number
        basis: string
        evidence_refs: string[]
      }> = []

      try {
        const jsonMatch = damageResponseText.match(/\[[\s\S]*\]/)
        if (jsonMatch) {
          damageList = JSON.parse(jsonMatch[0])
        }
      } catch {
        return { result: 'Error: ç„¡æ³•è§£æé‡‘é¡è¨ˆç®—çµæœ', success: false }
      }

      if (!damageList.length) {
        return { result: 'æœªèƒ½è­˜åˆ¥å‡ºè«‹æ±‚é‡‘é¡é …ç›®ï¼Œè«‹ç¢ºèªæª”æ¡ˆå·²æ­£ç¢ºè™•ç†ã€‚', success: false }
      }

      // 4. Clear old damages for this case, then write new ones
      await drizzle.delete(damages).where(eq(damages.case_id, caseId))
      const damageRecords = damageList.map((d) => ({
        id: nanoid(),
        case_id: caseId,
        category: d.category,
        description: d.description || null,
        amount: d.amount,
        basis: d.basis || null,
        evidence_refs: JSON.stringify(d.evidence_refs || []),
        dispute_id: null,
        created_at: new Date().toISOString(),
      }))

      for (const record of damageRecords) {
        await drizzle.insert(damages).values(record)
      }

      // 5. Send SSE brief_update with set_damages
      const damageData = damageRecords.map((r) => ({
        ...r,
        evidence_refs: JSON.parse(r.evidence_refs),
      }))

      await ctx.sendSSE({
        type: 'brief_update',
        brief_id: '',
        action: 'set_damages',
        data: damageData,
      })

      // 6. Return summary
      const totalAmount = damageRecords.reduce((sum, d) => sum + d.amount, 0)
      const damageSummary = damageRecords
        .map((d) => `- ${d.category}ï¼šNT$ ${d.amount.toLocaleString()}`)
        .join('\n')

      return {
        result: `å·²è¨ˆç®— ${damageRecords.length} é …é‡‘é¡ï¼š\n${damageSummary}\n\nè«‹æ±‚ç¸½é¡ï¼šNT$ ${totalAmount.toLocaleString()}`,
        success: true,
      }
    }

    case 'search_law': {
      if (!ctx) {
        return { result: 'Error: missing execution context', success: false }
      }

      const query = args.query as string
      const limit = (args.limit as number) || 10
      if (!query) {
        return { result: 'Error: query is required', success: false }
      }

      const results = await searchLaw(ctx.mongoUrl, { query, limit })

      if (results.length === 0) {
        return { result: `æœªæ‰¾åˆ°èˆ‡ã€Œ${query}ã€ç›¸é—œçš„æ³•æ¢ã€‚`, success: true }
      }

      // Upsert into D1 law_refs table
      for (const r of results) {
        try {
          const existing = await drizzle
            .select()
            .from(lawRefs)
            .where(eq(lawRefs.id, r._id))
          if (existing.length > 0) {
            await drizzle
              .update(lawRefs)
              .set({ usage_count: (existing[0].usage_count || 0) + 1 })
              .where(eq(lawRefs.id, r._id))
          } else {
            await drizzle.insert(lawRefs).values({
              id: r._id,
              case_id: caseId,
              law_name: r.law_name,
              article: r.article_no,
              title: `${r.law_name} ${r.article_no}`,
              full_text: r.content,
              usage_count: 1,
            })
          }
        } catch { /* skip on error */ }
      }

      // Read all law_refs for this case to send to frontend
      const allRefs = await drizzle
        .select()
        .from(lawRefs)
        .where(eq(lawRefs.case_id, caseId))

      await ctx.sendSSE({
        type: 'brief_update',
        brief_id: '',
        action: 'set_law_refs',
        data: allRefs,
      })

      // Format result text
      const formatted = results
        .map((r) => `${r.law_name} ${r.article_no}ï¼š${r.content.slice(0, 80)}${r.content.length > 80 ? '...' : ''}`)
        .join('\n')

      return {
        result: `æ‰¾åˆ° ${results.length} æ¢ç›¸é—œæ³•æ¢ï¼š\n${formatted}`,
        success: true,
      }
    }

    case 'generate_timeline': {
      if (!ctx) {
        return { result: 'Error: missing execution context', success: false }
      }

      // Load all ready files with summaries
      const timelineFileRows = await drizzle
        .select({
          id: files.id,
          filename: files.filename,
          category: files.category,
          doc_date: files.doc_date,
          summary: files.summary,
        })
        .from(files)
        .where(eq(files.case_id, caseId))

      const timelineReadyFiles = timelineFileRows.filter((f) => f.summary)
      if (!timelineReadyFiles.length) {
        return { result: 'æ²’æœ‰å·²è™•ç†å®Œæˆçš„æª”æ¡ˆï¼Œè«‹å…ˆä¸Šå‚³ä¸¦ç­‰å¾…æª”æ¡ˆè™•ç†å®Œç•¢ã€‚', success: false }
      }

      const fileContext = timelineReadyFiles.map((f) => {
        const summary = f.summary ? JSON.parse(f.summary) : {}
        return `ã€${f.filename}ã€‘(${f.category})\næ—¥æœŸï¼š${f.doc_date || 'ä¸æ˜'}\næ‘˜è¦ï¼š${summary.summary || 'ç„¡'}`
      }).join('\n\n')

      const timelinePrompt = `ä½ æ˜¯å°ˆæ¥­çš„å°ç£æ³•å¾‹åˆ†æåŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹æ¡ˆä»¶æ–‡ä»¶æ‘˜è¦ï¼Œç”¢ç”Ÿæ™‚é–“è»¸äº‹ä»¶åˆ—è¡¨ã€‚

${fileContext}

è«‹ä»¥ JSON æ ¼å¼å›å‚³æ™‚é–“è»¸äº‹ä»¶åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
  {
    "date": "2024-01-15",
    "title": "äº‹ä»¶æ¨™é¡Œ",
    "description": "äº‹ä»¶è©³ç´°æè¿°",
    "source_file": "ä¾†æºæª”æ¡ˆåç¨±",
    "is_critical": true
  }
]

è¦å‰‡ï¼š
- date æ ¼å¼ç‚º YYYY-MM-DDï¼Œè‹¥åªçŸ¥å¹´æœˆå‰‡ç‚º YYYY-MM-01ï¼Œè‹¥åªçŸ¥å¹´å‰‡ç‚º YYYY-01-01
- is_critical ç‚ºå¸ƒæ—å€¼ï¼Œæ¨™è¨˜é—œéµäº‹ä»¶ï¼ˆå¦‚èµ·è¨´ã€åˆ¤æ±ºã€ç°½ç´„ã€é•ç´„ç­‰ï¼‰
- æŒ‰æ—¥æœŸå¾æ—©åˆ°æ™šæ’åº
- é‡è¦ï¼šçµ•å°ä¸è¦ä½¿ç”¨ emoji æˆ–ç‰¹æ®Šç¬¦è™Ÿ
- åªå›å‚³ JSON é™£åˆ—ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚`

      const timelineAiResponse = await callAIStreaming(ctx.aiEnv, {
        messages: [
          { role: 'system', content: 'ä½ æ˜¯å°ˆæ¥­çš„å°ç£æ³•å¾‹åˆ†æåŠ©æ‰‹ã€‚' },
          { role: 'user', content: timelinePrompt },
        ],
      })

      const timelineResponseText = await collectStreamText(timelineAiResponse)

      let timelineEvents: Array<{
        date: string
        title: string
        description: string
        source_file: string
        is_critical: boolean
      }> = []

      try {
        const jsonMatch = timelineResponseText.match(/\[[\s\S]*\]/)
        if (jsonMatch) {
          timelineEvents = JSON.parse(jsonMatch[0])
        }
      } catch {
        return { result: 'Error: ç„¡æ³•è§£ææ™‚é–“è»¸çµæœ', success: false }
      }

      if (!timelineEvents.length) {
        return { result: 'æœªèƒ½å¾æª”æ¡ˆä¸­è­˜åˆ¥å‡ºæ™‚é–“è»¸äº‹ä»¶ã€‚', success: false }
      }

      // Sort by date
      timelineEvents.sort((a, b) => a.date.localeCompare(b.date))

      // Persist to D1 â€” delete old events for this case, then insert new ones
      await drizzle.delete(timelineEventsTable).where(eq(timelineEventsTable.case_id, caseId))
      const now = new Date().toISOString()
      for (const evt of timelineEvents) {
        await drizzle.insert(timelineEventsTable).values({
          id: nanoid(),
          case_id: caseId,
          date: evt.date,
          title: evt.title,
          description: evt.description || '',
          source_file: evt.source_file || '',
          is_critical: evt.is_critical || false,
          created_at: now,
        })
      }

      // Send via SSE
      await ctx.sendSSE({
        type: 'brief_update',
        brief_id: '',
        action: 'set_timeline',
        data: timelineEvents,
      })

      const timelineSummary = timelineEvents
        .map((e) => `${e.date} ${e.title}${e.is_critical ? ' (é—œéµ)' : ''}`)
        .join('\n')

      return {
        result: `å·²ç”¢ç”Ÿ ${timelineEvents.length} å€‹æ™‚é–“è»¸äº‹ä»¶ï¼š\n${timelineSummary}`,
        success: true,
      }
    }

    default:
      return { result: `Unknown tool: ${toolName}`, success: false }
  }
}
